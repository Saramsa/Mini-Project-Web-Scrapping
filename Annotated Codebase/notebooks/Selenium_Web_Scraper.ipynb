{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03bb2eec-ba56-4029-81b2-2d0f023372d0",
   "metadata": {},
   "source": [
    "## Importing Required Libraries\n",
    "\n",
    "### undetected_chromedriver\n",
    "- **Purpose:** A special version of ChromeDriver that helps avoid detection by websites which may block automated browsing.\n",
    "- **Use case:** Ensures the browser automation mimics real user behavior to reduce the chance of being blocked.\n",
    "\n",
    "### selenium.webdriver.common.by – By\n",
    "- **Purpose:** Locate elements on a webpage (by ID, XPath, class name, etc.).\n",
    "- **Use case:** Essential for finding specific HTML elements to interact with during automation.\n",
    "\n",
    "### selenium.webdriver.support.ui – WebDriverWait\n",
    "- **Purpose:** Wait for certain conditions or elements to be ready before continuing execution.\n",
    "- **Use case:** Prevents errors by ensuring elements have loaded or become interactive before acting.\n",
    "\n",
    "### selenium.webdriver.support.expected_conditions – EC\n",
    "- **Purpose:** A collection of pre-built conditions (e.g., element clickable, visible).\n",
    "- **Use case:** Works with `WebDriverWait` to define what to wait for.\n",
    "\n",
    "### traceback\n",
    "- **Purpose:** Extract, format, and print stack traces.\n",
    "- **Use case:** Debugging and error logging during script execution.\n",
    "\n",
    "### time\n",
    "- **Purpose:** Provides time-related functions like `sleep()`.\n",
    "- **Use case:** Pausing execution for set durations to control scraping pace.\n",
    "\n",
    "### random\n",
    "- **Purpose:** Generates pseudo-random numbers.\n",
    "- **Use case:** Introduces randomness (e.g., variable wait times) to mimic human activity.\n",
    "\n",
    "### re\n",
    "- **Purpose:** Regular expression operations for pattern matching in strings.\n",
    "- **Use case:** Extracting or validating text patterns such as prices or dates.\n",
    "\n",
    "### csv\n",
    "- **Purpose:** Read from and write to CSV files.\n",
    "- **Use case:** Store scraped data in a structured, tabular format.\n",
    "\n",
    "### urllib.parse\n",
    "- **Purpose:** Manipulate URLs (encode/decode query parameters, parse components).\n",
    "- **Use case:** Construct or deconstruct URLs during navigation or data extraction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5cbc7ce-00ce-4a3e-9371-591ff9b9d5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the undetected_chromedriver module as 'uc'.\n",
    "# This is a special version of ChromeDriver that helps to avoid detection by websites\n",
    "# which might block or restrict automated browser actions.\n",
    "import undetected_chromedriver as uc\n",
    "\n",
    "# Importing By class from selenium.webdriver.common.by\n",
    "# 'By' is used to locate elements on a webpage, such as by ID, XPATH, class name, etc.\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Importing WebDriverWait class from selenium.webdriver.support.ui\n",
    "# WebDriverWait allows us to wait for certain conditions or elements to be ready \n",
    "# before proceeding with the next steps in automation.\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "\n",
    "# Importing expected_conditions as EC from selenium.webdriver.support\n",
    "# expected_conditions provides a set of pre-built conditions to use with WebDriverWait,\n",
    "# e.g., waiting for an element to be clickable or visible.\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Importing traceback module\n",
    "# This module provides utilities to extract, format, and print stack traces of Python programs.\n",
    "# Useful for debugging errors and exceptions.\n",
    "import traceback\n",
    "\n",
    "# Importing time module\n",
    "# Provides various time-related functions such as sleep(), which pauses execution for a given number of seconds.\n",
    "import time\n",
    "\n",
    "# Importing random module\n",
    "# This module implements pseudo-random number generators for various distributions.\n",
    "# Often used to introduce randomness, e.g., random wait times to mimic human behavior.\n",
    "import random\n",
    "\n",
    "# Importing re module\n",
    "# This module provides regular expression matching operations, allowing pattern matching in strings.\n",
    "import re\n",
    "\n",
    "# Importing csv module\n",
    "# Used for reading from and writing to CSV (Comma-Separated Values) files easily.\n",
    "import csv\n",
    "\n",
    "# Importing urllib.parse module\n",
    "# Contains functions to manipulate URLs, such as encoding query parameters or parsing URL components.\n",
    "import urllib.parse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c73076-ddcf-439f-a9cd-88299d707ece",
   "metadata": {},
   "source": [
    "### Function: `get_coupons_with_retry`\n",
    "\n",
    "This function automates the process of retrieving coupon prices from a GoodRx webpage using Selenium.\n",
    "\n",
    "**Purpose:**\n",
    "- Locate and extract two coupon prices from dynamically loaded page elements.\n",
    "- Retry multiple times if coupons are not immediately visible, reducing the chance of missing data due to slow loading.\n",
    "\n",
    "**Key Features:**\n",
    "- Uses `WebDriverWait` with a short timeout to avoid long script freezes.\n",
    "- Scrolls to coupon elements to ensure they are in view before extraction.\n",
    "- Handles missing or empty text values by returning `\"N/A\"`.\n",
    "- Includes a retry mechanism with a configurable number of attempts and delays between retries.\n",
    "\n",
    "**Parameters:**\n",
    "- `driver`: Selenium WebDriver controlling the browser session.\n",
    "- `wait`: WebDriverWait instance for explicit waits.\n",
    "- `retries`: Number of times to attempt retrieval before failing.\n",
    "- `delay`: Pause duration (seconds) between retries.\n",
    "\n",
    "**Returns:**\n",
    "- Tuple containing two coupon prices as strings.\n",
    "- Returns `(\"N/A\", \"N/A\")` if prices are not retrieved after all retries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8600315-b7b9-4849-a851-a1ce34ea7abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "def get_coupons_with_retry(driver, wait, retries=3, delay=2):\n",
    "    \"\"\"\n",
    "    Attempts to retrieve coupon prices from the webpage, retrying if elements are not found or visible.\n",
    "    \n",
    "    Parameters:\n",
    "    - driver: Selenium WebDriver instance controlling the browser.\n",
    "    - wait: WebDriverWait instance for handling explicit waits.\n",
    "    - retries: Number of times to retry finding the coupon elements before giving up (default 3).\n",
    "    - delay: Number of seconds to wait between retries (default 2).\n",
    "    \n",
    "    Returns:\n",
    "    - Tuple of two strings representing coupon prices if found.\n",
    "    - Returns (\"N/A\", \"N/A\") if coupons could not be retrieved after all retries.\n",
    "    \"\"\"\n",
    "    \n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            # **Use short timeout to avoid long freeze:** wait.max_wait typically controls timeout,\n",
    "            # but explicitly using wait here is risky if wait has a large timeout.\n",
    "            # Instead, use WebDriverWait with a short timeout inline to avoid freezing.\n",
    "            \n",
    "            # Wait up to 5 seconds for the elements to be visible\n",
    "            coupons = WebDriverWait(driver, 5).until(\n",
    "                EC.visibility_of_all_elements_located((By.CSS_SELECTOR, \"span[data-qa='pricing-option-price']\"))\n",
    "            )\n",
    "            \n",
    "            # Once visible elements are found\n",
    "            if len(coupons) >= 2:\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", coupons[0])\n",
    "                time.sleep(1)  # Allow for animations/item loads to finish\n",
    "                \n",
    "                # Clean the coupon text; if empty, treat as \"N/A\"\n",
    "                coupon1 = coupons[0].text.strip() if coupons[0].text.strip() else \"N/A\"\n",
    "                coupon2 = coupons[1].text.strip() if coupons[1].text.strip() else \"N/A\"\n",
    "                \n",
    "                return coupon1, coupon2\n",
    "            \n",
    "            else:\n",
    "                # Not enough coupon elements found yet, retry after delay\n",
    "                time.sleep(delay)\n",
    "        except Exception as e:\n",
    "            # Catch specific exceptions if you prefer (TimeoutException, NoSuchElementException)\n",
    "            # but broad except keeps it safe here.\n",
    "            \n",
    "            # Optional: log the exception for debugging\n",
    "            # print(f\"[Warning] Attempt {attempt + 1}: Exception when fetching coupons: {e}\")\n",
    "            \n",
    "            time.sleep(delay)\n",
    "    \n",
    "    # After all retries exhausted, return N/A\n",
    "    return \"N/A\", \"N/A\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23886d8f-8a6e-44b7-b97c-b9d5bfe5bfc7",
   "metadata": {},
   "source": [
    "### Function: `scrape_goodrx`\n",
    "\n",
    "Automates the process of scraping GoodRx pharmacy pricing information for a given drug and ZIP code.\n",
    "\n",
    "**Purpose:**\n",
    "- Open a drug’s GoodRx page in a Chrome browser (undetected mode).\n",
    "- Enter a specified ZIP code to update location-based prices.\n",
    "- Extract pharmacy details, prices, dosages, quantities, and special offers.\n",
    "- Retrieve coupon prices for listings with special offers.\n",
    "\n",
    "**Key Features:**\n",
    "- **Browser automation:** Uses `undetected_chromedriver` to avoid bot detection.\n",
    "- **Dynamic element handling:** Waits for and interacts with modal dialogs, inputs, and buttons.\n",
    "- **Data extraction:** Collects drug details, pharmacy names, standard prices, special offers, and coupon prices.\n",
    "- **Fallback handling:** Assigns `\"N/A\"` where data is missing.\n",
    "- **Retry mechanism:** Uses `get_coupons_with_retry` to fetch coupon prices reliably.\n",
    "- **Graceful termination:** Closes browser even if errors occur.\n",
    "\n",
    "**Parameters:**\n",
    "- `drug_url` *(str)*: Full GoodRx URL for the target drug.\n",
    "- `zip_code` *(str)*: ZIP code to use for location-based pricing.\n",
    "\n",
    "**Returns:**\n",
    "- **List[dict]**: Each dictionary contains:\n",
    "  - `drug`, `location`, `dosage`, `quantity`, `pharmacy`, `price`,  \n",
    "    `special_offer`, `standard_coupon_price`, `special_coupon_price`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7048c0d9-a413-43c6-b050-6eeba602dfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_goodrx(drug_url, zip_code):\n",
    "    scraped_results = []  # Will hold all scraped data entries\n",
    "\n",
    "    # Parse drug name from URL path, capitalize first letter\n",
    "    parsed_url = urllib.parse.urlparse(drug_url)\n",
    "    drug_name_from_url = parsed_url.path.strip(\"/\").split(\"/\")[-1]\n",
    "    drug_name_formatted = drug_name_from_url.capitalize()  # e.g. 'atorvastatin'\n",
    "\n",
    "    try:\n",
    "        # Initialize Chrome WebDriver options to start maximized for visual stability\n",
    "        options = uc.ChromeOptions()\n",
    "        options.add_argument(\"--start-maximized\")\n",
    "\n",
    "        # Create undetected chromedriver instance (to avoid easy detection)\n",
    "        driver = uc.Chrome(options=options)\n",
    "        driver.delete_all_cookies()\n",
    "\n",
    "        # Navigate to drug URL\n",
    "        driver.get(drug_url)\n",
    "        time.sleep(random.uniform(3, 5))  # Random delay to mimic human browsing\n",
    "        print(f\"✅ Page loaded: {driver.title}\")\n",
    "\n",
    "        wait = WebDriverWait(driver, 30)  # Explicit wait setup\n",
    "\n",
    "        # Step 1: Click location selector to enter ZIP code\n",
    "        location_span = wait.until(EC.element_to_be_clickable((By.XPATH, \"//span[contains(@class,'truncate')]\")))\n",
    "        location_span.click()\n",
    "        time.sleep(2)  # Allow modal to appear\n",
    "\n",
    "        # Step 2: Input ZIP code and submit\n",
    "        zip_input = wait.until(EC.presence_of_element_located((By.ID, \"locationModalAddress\")))\n",
    "        zip_input.clear()\n",
    "        zip_input.send_keys(zip_code)\n",
    "\n",
    "        submit_button = wait.until(EC.element_to_be_clickable(\n",
    "            (By.XPATH, \"//button[@type='submit' and @form='locationModalForm' and contains(translate(text(),'ABCDEFGHIJKLMNOPQRSTUVWXYZ','abcdefghijklmnopqrstuvwxyz'),'set location')]\")\n",
    "        ))\n",
    "        submit_button.click()\n",
    "        time.sleep(6)  # Wait longer for page to update prices by location\n",
    "\n",
    "        # Find all pharmacy cards displayed on the page\n",
    "        pharmacy_cards = driver.find_elements(By.CSS_SELECTOR, \"li.list-none.flex.shadow-raised\")\n",
    "        print(f\"ℹ️ Found {len(pharmacy_cards)} pharmacy cards for ZIP {zip_code}\")\n",
    "\n",
    "        # Iterate over each pharmacy card to extract details\n",
    "        for i, card in enumerate(pharmacy_cards):\n",
    "            try:\n",
    "                # Scroll pharmacy card into view smoothly\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView({behavior: 'smooth', block: 'center'});\", card)\n",
    "                time.sleep(1.5)  # pause to stabilize rendering\n",
    "\n",
    "                # Extract drug info dynamically by locating the span containing the drug name\n",
    "                try:\n",
    "                    xpath_str = f\"//span[contains(text(),'{drug_name_formatted}')]\"\n",
    "                    drug_info = driver.find_element(By.XPATH, xpath_str).text.strip()\n",
    "                    pattern = rf\"({re.escape(drug_name_formatted)})\\s+(\\d+mg)\\s+\\((\\d+\\s+\\w+)\\)\"\n",
    "                    match = re.match(pattern, drug_info)\n",
    "                    if match:\n",
    "                        drug_name, dosage, quantity = match.groups()\n",
    "                    else:\n",
    "                        drug_name, dosage, quantity = \"N/A\", \"N/A\", \"N/A\"\n",
    "                except:\n",
    "                    drug_name, dosage, quantity = \"N/A\", \"N/A\", \"N/A\"\n",
    "\n",
    "                # Extract pharmacy name\n",
    "                try:\n",
    "                    pharmacy = card.find_element(By.CSS_SELECTOR, \"span[data-qa='seller-name']\").text.strip()\n",
    "                except:\n",
    "                    pharmacy = \"N/A\"\n",
    "\n",
    "                # Extract price listed\n",
    "                try:\n",
    "                    price = card.find_element(By.CSS_SELECTOR, \"span[data-qa='seller-price']\").text.strip()\n",
    "                except:\n",
    "                    price = \"N/A\"\n",
    "\n",
    "                # Extract special offer description\n",
    "                try:\n",
    "                    offer = card.find_element(By.CSS_SELECTOR, \"span[data-qa='special-offer-text']\").text.strip()\n",
    "                except:\n",
    "                    offer = \"No special offer\"\n",
    "\n",
    "                # Handle cards without special offers differently - just append data\n",
    "                if offer.lower() == \"no special offer\":\n",
    "                    scraped_results.append({\n",
    "                        \"drug\": drug_name,\n",
    "                        \"location\": zip_code,\n",
    "                        \"dosage\": dosage,\n",
    "                        \"quantity\": quantity,\n",
    "                        \"pharmacy\": pharmacy,\n",
    "                        \"price\": price,\n",
    "                        \"special_offer\": offer,\n",
    "                        \"standard_coupon_price\": \"N/A\",\n",
    "                        \"special_coupon_price\": \"N/A\",\n",
    "                    })\n",
    "                    print(f\"Listed (No Special Offer): {pharmacy} at {zip_code}\")\n",
    "                    print(\"-\" * 50)\n",
    "                    continue\n",
    "\n",
    "                # For cards with special offers, click to load coupon prices\n",
    "                try:\n",
    "                    card.click()\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                # Use your get_coupons_with_retry function to retrieve coupon prices\n",
    "                standard, special = get_coupons_with_retry(driver, wait)\n",
    "\n",
    "                scraped_results.append({\n",
    "                    \"drug\": drug_name,\n",
    "                    \"location\": zip_code,\n",
    "                    \"dosage\": dosage,\n",
    "                    \"quantity\": quantity,\n",
    "                    \"pharmacy\": pharmacy,\n",
    "                    \"price\": price,\n",
    "                    \"special_offer\": offer,\n",
    "                    \"standard_coupon_price\": standard,\n",
    "                    \"special_coupon_price\": special,\n",
    "                })\n",
    "\n",
    "                print(f\"Scraped: {pharmacy} at {zip_code} (Special Offer)\")\n",
    "                print(\"-\" * 50)\n",
    "\n",
    "            except Exception:\n",
    "                print(f\"❌ Error on card {i+1} at {zip_code}\")\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "\n",
    "    except Exception:\n",
    "        print(\"❌ Critical failure during scrape\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "    finally:\n",
    "        try:\n",
    "            driver.quit()\n",
    "            print(\"🔚 Browser closed\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return scraped_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e75f4f0-07c2-4054-9e6c-77f07256a257",
   "metadata": {},
   "source": [
    "### Target Drug URLs and ZIP Codes\n",
    "\n",
    "**Drug URLs:**\n",
    "A predefined list of GoodRx drug pages to scrape.  \n",
    "Each URL points to a specific drug’s pricing and offers page.\n",
    "\n",
    "**ZIP Codes:**\n",
    "A list of U.S. ZIP codes used for location-based pricing lookups.  \n",
    "Scraping for multiple ZIP codes allows price comparison across different regions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "939036d1-477c-42bd-9d74-36fe76475975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of URLs for different drugs on the GoodRx website\n",
    "# Each URL corresponds to a specific drug page to scrape pricing and offers from\n",
    "drug_urls = [\n",
    "    \"https://www.goodrx.com/atorvastatin\",\n",
    "    \"https://www.goodrx.com/lisinopril\",\n",
    "    \"https://www.goodrx.com/metformin\",\n",
    "    \"https://www.goodrx.com/amlodipine\",\n",
    "    \"https://www.goodrx.com/omeprazole\",\n",
    "    \"https://www.goodrx.com/simvastatin\",\n",
    "    \"https://www.goodrx.com/losartan\",\n",
    "    \"https://www.goodrx.com/albuterol\",\n",
    "    \"https://www.goodrx.com/hydrochlorothiazide\",\n",
    "    \"https://www.goodrx.com/levothyroxine\",\n",
    "]\n",
    "\n",
    "\n",
    "# List of ZIP codes to specify location-based price lookups\n",
    "# Each ZIP code corresponds to a geographical area in the US to emulate user location\n",
    "zip_codes = [\n",
    "    \"10001\",  # ZIP code for New York, NY\n",
    "    \"90001\",  # ZIP code for Los Angeles, CA\n",
    "    \"60601\",  # ZIP code for Chicago, IL\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb04420-b3d7-406c-9611-7169d2296143",
   "metadata": {},
   "source": [
    "### Scraping Loop – Multiple Drugs and Locations\n",
    "\n",
    "This section runs the scraping process for every combination of target drug and ZIP code.\n",
    "\n",
    "**Process:**\n",
    "1. Iterate over each drug URL in `drug_urls`.\n",
    "2. For each drug, iterate over each ZIP code in `zip_codes`.\n",
    "3. Call `scrape_goodrx()` to retrieve pricing and offer data for that drug-location pair.\n",
    "4. Append results to `all_scraped_data`, a master list storing all collected records.\n",
    "5. Pause for 10 seconds between requests to avoid overwhelming the GoodRx servers and reduce the risk of being blocked.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3df98e89-fa18-4e53-ad20-f317234bcb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to collect all scraped data from multiple drug and ZIP code combinations\n",
    "all_scraped_data = []\n",
    "\n",
    "# Outer loop: iterate over each drug URL in the drug_urls list\n",
    "for drug_url in drug_urls:\n",
    "    # Inner loop: iterate over each ZIP code in the zip_codes list for location-specific scraping\n",
    "    for zip_code in zip_codes:\n",
    "        # Print a message to indicate which drug URL and ZIP code is currently being scraped\n",
    "        print(f\"\\n=== Scraping {drug_url} for ZIP {zip_code} ===\")\n",
    "        \n",
    "        # Call the scrape_goodrx function to scrape pricing and offer data for the specific drug and location\n",
    "        data = scrape_goodrx(drug_url, zip_code)\n",
    "        \n",
    "        # Append the scraped data (a list of dictionaries) to the master all_scraped_data list\n",
    "        all_scraped_data.extend(data)\n",
    "        \n",
    "        # Pause execution for 10 seconds to avoid overwhelming the target website\n",
    "        # This polite delay reduces the risk of being blocked or flagged as a bot\n",
    "        time.sleep(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34660195-2462-4d5f-a58a-8f70bf3e14a3",
   "metadata": {},
   "source": [
    "### Saving Scraped Data to CSV\n",
    "\n",
    "After completing the scraping process, this section saves the collected data to a CSV file.\n",
    "\n",
    "**Process:**\n",
    "1. Check if `all_scraped_data` contains any records.\n",
    "2. Use the keys from the first record as column headers.\n",
    "3. Create a new file named `scraped_goodrx_data.csv` with UTF-8 encoding.\n",
    "4. Write the header row followed by all scraped records.\n",
    "5. Print a confirmation message showing the total number of records saved.\n",
    "6. If no data was scraped, print a message indicating the dataset is empty.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9124ffef-b4dd-40ef-9bf7-310fa6fb4595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 345 records to scraped_goodrx_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Check if the list 'all_scraped_data' contains any scraped data\n",
    "if all_scraped_data:\n",
    "    \n",
    "    # Get the dictionary keys from the first item to use as CSV column headers\n",
    "    keys = all_scraped_data[0].keys()\n",
    "    \n",
    "    # Open a new CSV file named \"scraped_goodrx_data.csv\" in write mode\n",
    "    # 'newline=\"\"' prevents adding extra blank lines on some platforms\n",
    "    # 'encoding=\"utf-8\"' ensures proper character encoding support\n",
    "    with open(\"scraped_goodrx_data.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "        \n",
    "        # Create a CSV DictWriter object using the keys as fieldnames (column headers)\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=keys)\n",
    "        \n",
    "        # Write the header row to the CSV file\n",
    "        writer.writeheader()\n",
    "        \n",
    "        # Write all the dictionaries in 'all_scraped_data' as rows in the CSV file\n",
    "        writer.writerows(all_scraped_data)\n",
    "    \n",
    "    # Print a confirmation message including the number of records saved\n",
    "    print(f\"Saved {len(all_scraped_data)} records to scraped_goodrx_data.csv\")\n",
    "\n",
    "else:\n",
    "    # If 'all_scraped_data' is empty, print a message to indicate no data was scraped\n",
    "    print(\"No data scraped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c447fc-bb4c-4cd1-afb6-bb7b5d97de36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
